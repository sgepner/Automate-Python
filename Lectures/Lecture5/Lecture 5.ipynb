{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5\n",
    "Serialization of data and use of external application API (Application Programmable Interface)\n",
    "\n",
    "Often Python scripts are used to process data in consequence creating new data for further use. The processing part can itself be computationally involving and being able to store data in the easily accessible form is very convenient. Especially so if the processing part takes days (but even hours) and access to data is required at some latter stages of the project. One, obvious solution is to store resulting data in simple CSV type files, but this approach is limited, especially if data structure becomes complex.\n",
    "For example, we might be storing time history of velocity profiles $(u,v,w)$ through a flow field retrieved from a number of locations and performed across snapshots in time ... . In general we have a lot of complex data to deal with, and would like to store it in some organized manner, to be retrieved and worked on latter. For this we can use **serialization**, which in the simplest terms means to store data. The concept comes from the need to communicate different application forming a system and working possibly across different platforms (or simply form game saves ...).\n",
    "\n",
    "Here we will illustrate how to **store** and **retrieve** data we process and produce. Consider a process where we have a number of snapshots from computations (data could be data from experiments) describing some arbitrary vector field in time (or positions of a robot, some mappings of its surroundings, readings from sensors etc.). We will store such data within objects of the following class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Case:\n",
    "    def __init__(self, fname):\n",
    "        self.file_name = fname # File name from which we will read\n",
    "        \n",
    "        self.time = 0 # solution time\n",
    "        self.number_of_pointst = 0 # number of data points\n",
    "\n",
    "        self.profile_pos = np.array([0,0,0]) # position of the profile cut\n",
    "\n",
    "        self.Y = np.array # Y coordinate\n",
    "        self.u = np.array # Components of velocity acros Y\n",
    "        self.v = np.array\n",
    "        self.w = np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a simple class that will hold $(u,v,w)$ components of a vector obtained at $time$ and at a point with $Y$ the coordinate across the cut. Lets now populate it with some made up data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clist = []\n",
    "c = Case(\"file_0.dat\") # some file we read from\n",
    "c.time = 0\n",
    "c.number_of_pointst = 1000\n",
    "c.profile_pos[0] = 5 # move at x=5\n",
    "c.profile_pos[1] = 10 # move at y=5\n",
    "c.Y = np.linspace(-1,1,100)\n",
    "# Normally u,v,w are read from file, here made up\n",
    "c.u = 1-c.Y**2\n",
    "c.v = c.Y * 0\n",
    "c.w = c.Y * 0\n",
    "clist.append(c) # add to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(c.u, c.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let have a second one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Case(\"filw_1.dat\") # some file we read from\n",
    "c.time = 1\n",
    "c.number_of_pointst = 1000\n",
    "c.profile_pos[0] = 5 # move at x=5\n",
    "c.profile_pos[1] = 10 # move at y=5\n",
    "c.Y = np.linspace(-1,1,100)\n",
    "# Normally u,v,w are read from file, here made up\n",
    "c.u = 1-c.Y**3\n",
    "c.v = c.Y * 0\n",
    "c.w = c.Y * 0\n",
    "clist.append(c) # add to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in clist:\n",
    "    plt.plot(c.u, c.Y)\n",
    "plt.xlim(0,2)\n",
    "plt.ylim(-1,1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usually there will be more data, the structure might be more complex, or some additional process will be used to do something with the data etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data storage we will use the `msgpack`, which is a serialization format for use in data exchange across different languages, but not only. Here is the project:  \n",
    "https://github.com/msgpack/msgpack-python  \n",
    "and its API documentation:  \n",
    "https://msgpack-python.readthedocs.io/en/latest/api.html  \n",
    "\n",
    "In short, hands on How to Use we will require an instruction on how to handle data. How to encode and pack it and how to unpack and decode it. So we need to add encoding as well as decoding functions. Those function will accept an object of our data storage class and inform `msgpack` what to do with it. The whole process is in fact quite simple. We just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import an appropriate module, and:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(clist[0]) == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Encoding function\"\"\"\n",
    "def enc_full(obj):\n",
    "    if type(obj) == Case: # make sure obj is of some type\n",
    "        code = 1 # A flag determining the type\n",
    "        # pack and return\n",
    "        return msgpack.ExtType(code, msgpack.packb([obj.file_name, obj.time, obj.number_of_pointst,\n",
    "                                             obj.profile_pos.tostring(),\n",
    "                                             obj.Y.tostring(),\n",
    "                                             obj.u.tostring(), obj.v.tostring(), obj.w.tostring()]))\n",
    "    raise TypeError # Rise an error if type not handled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have the encoding part. Now, all simple data types go as they are (file_name, time ...) and Numpy arrays need to be converted to strings. The `packb` simply packs an object into an appropriate format. One comment here is that we are not packing the `Case` object itself, but rather a list (note `[ ]`) of attributes such object has. The `code` identifies the stored type, since we could have more than one class to deal with the `code` would let us distinguish recovered data.\n",
    "We can now store this data to a file, simply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgpack.dump(clist, open( 'aaa.dat', \"wb\" ), default=enc_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `\"wb\"` lets us store this data in binary format. We will need to recover the data appropriatly with `\"rb\"`. See if the file was created, and look inside.\n",
    "\n",
    "Now restart the Python interpreter and try to recover the data we put on disk. The `Case` class still needs to be defined, so in case you restarted the interpreter, do run the appropriate cell.  \n",
    "To decode we will need a decoding function. It will contain instructions on how to rebuild objects from storage, create an object, feed it with data and return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msgpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnc(code, data):\n",
    "    if code == 1: # a flag determining the type\n",
    "        # call the unpack on the data passed in here\n",
    "        file_name, time, num_pts, pos, Y, u, v, w = msgpack.unpackb(data)\n",
    "        \n",
    "        # make the object\n",
    "        c = Case(file_name)\n",
    "        \n",
    "        # feed it with data\n",
    "        c.time = time\n",
    "        c.number_of_pointst = num_pts\n",
    "        # Note that numpy arrays need to be handled in a speciall way\n",
    "        c.profile_pos = np.frombuffer(pos)\n",
    "        c.Y = np.frombuffer(Y)\n",
    "        c.u = np.frombuffer(u)\n",
    "        c.v = np.frombuffer(v)\n",
    "        c.w = np.frombuffer(w)\n",
    "        \n",
    "        # return the object outside\n",
    "        return c\n",
    "    # in case the type is either a build in or other\n",
    "    return msgpack.ExtType(code, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just one class, and its code is simply `1`. `dnc()` will be passed this `code` as well as the binary data, which will be unpacked and returned as a tuple.\n",
    "It is up to us to create a new object and populate it with data and than to return it.\n",
    "\n",
    "Lifting data from storage is now simple. Note that we need to pass the decoding function to be used by `load()`, the file name and `\"rb\"` since we read binary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_list = msgpack.load(open( 'aaa.dat', \"rb\" ), ext_hook=dnc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a copy of our data should be in `recovered_list`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in recovered_list:\n",
    "    plt.plot(c.u, c.Y)\n",
    "plt.xlim(0,2)\n",
    "plt.ylim(-1,1)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with API\n",
    "Various application provide some form of interface to interact with it. Python API becomes more and more a standard. How to connect to, how to use and to what it might be beneficial is very much problem dependant. For this I decided to show you connection to the Open Source visualization package ParaView. I use it to visualize CFD data, but it also used in other fields of engineering.\n",
    "\n",
    "So the first step is to get your hands on the API, it will come as a Python module, it might need to be compiled, or be available as a software package. Here, I have the module compiled to a library placed in `/opt/paraview/lib/python3.8/`. Also I know I will need some of the Numpy functionality to process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/paraview/lib/python3.8/site-packages/')\n",
    "sys.path.append('/opt/paraview/lib/python3.8/')\n",
    "sys.path.append('/opt/paraview/lib/')\n",
    "\n",
    "from paraview.simple import *\n",
    "from paraview.numeric import fromvtkarray\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from vtk.util.numpy_support import vtk_to_numpy\n",
    "from vtk.numpy_interface import dataset_adapter as dsa\n",
    "from vtk.numpy_interface import algorithms as algs\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be done with the API is very much up to the providing application and the level of documentation. In the case of Open Source project documentation is often lacking. This is the case with ParaView (was some time ago, could be better now). In what follows I will load a flow field snapshot and plot velocity profiles at selected positions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = VisItTecplotBinaryReader(FileName=['geom.plt'])\n",
    "geom.PointArrayStatus = ['u', 'v', 'w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_over_line = PlotOverLine(Input=geom, Source='High Resolution Line Source')\n",
    "plot_over_line.Source.Point1 = [0.0, -1.0, 12.0]\n",
    "plot_over_line.Source.Point2 = [0.0, 1.0, 12.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = servermanager.Fetch(plot_over_line)\n",
    "\n",
    "pos = fromvtkarray(pl.GetPoints().GetData())\n",
    "valsU = fromvtkarray(pl.GetPointData().GetScalars(\"u\"))\n",
    "valsU = np.hstack([val for val in valsU])\n",
    "valsV = fromvtkarray(pl.GetPointData().GetScalars(\"v\"))\n",
    "valsV = np.hstack([val for val in valsV])\n",
    "valsW = fromvtkarray(pl.GetPointData().GetScalars(\"w\"))\n",
    "valsW = np.hstack([val for val in valsW])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we have velocity components and positions, just what we need to make a nice plot. One of the things that one may, or not learn from documentation is that `pos` contains a transposed coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pos.T[1], valsU)\n",
    "plt.plot(pos.T[1], valsV)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pos.T[1], valsW)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can modify the direction data is taken from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_over_line.Source.Point1 = [0.0, 0.0, 0.0]\n",
    "plot_over_line.Source.Point2 = [0.0, 0.0, 25.13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = servermanager.Fetch(plot_over_line)\n",
    "\n",
    "pos = fromvtkarray(pl.GetPoints().GetData())\n",
    "valsU = fromvtkarray(pl.GetPointData().GetScalars(\"u\"))\n",
    "valsU = np.hstack([val for val in valsU])\n",
    "valsV = fromvtkarray(pl.GetPointData().GetScalars(\"v\"))\n",
    "valsV = np.hstack([val for val in valsV])\n",
    "valsW = fromvtkarray(pl.GetPointData().GetScalars(\"w\"))\n",
    "valsW = np.hstack([val for val in valsW])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plotting along $z$ coordinate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pos.T[2], valsW)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
